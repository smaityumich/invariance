{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sinkhorn as sh\n",
    "import numpy as np\n",
    "import utils5 as utils\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "import cProfile\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informative_feature(y):\n",
    "    return np.random.normal(1, 1, (4,)) if y else np.random.normal(0, 1, (4,))\n",
    "\n",
    "def spurious_label(y, p):\n",
    "    z = np.random.random()\n",
    "    return y if z<p else 1-y #np.random.binomial(1, 0.5) #1-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = np.random.binomial(1, 0.5, (1200,)) ##Bayes error 0.14\n",
    "y1 = np.random.binomial(1, 0.5, (1500,))\n",
    "#f = lambda y: np.random.normal(1, 1, (4,)) if y else np.random.normal(0, 1, (4,))\n",
    "x0_inv = [informative_feature(spurious_label(y, 0.25)) for y in y0]\n",
    "x1_inv = [informative_feature(spurious_label(y, 0.25)) for y in y1]\n",
    "x0_non_inv = [informative_feature(spurious_label(y, 0.2)) for y in y0]\n",
    "x1_non_inv = [informative_feature(spurious_label(y, 0.1)) for y in y1]\n",
    "x0 = np.concatenate((x0_inv, x0_non_inv), axis = 1)\n",
    "x1 = np.concatenate((x1_inv, x1_non_inv), axis = 1)\n",
    "#x0 = np.concatenate((x0, np.random.normal(6, 1, (1200, 60))), axis = 1)\n",
    "#x1 = np.concatenate((x1, np.random.normal(2, 1, (1500, 60))), axis = 1)\n",
    "y0 = tf.one_hot(y0, 2)\n",
    "y1 = tf.one_hot(y1, 2)\n",
    "x0 = tf.cast(x0, dtype = tf.float32)\n",
    "x1 = tf.cast(x1, dtype = tf.float32)\n",
    "\n",
    "data_train = [[x0, y0], [x1, y1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = np.random.binomial(1, 0.5, (1000,))\n",
    "y1 = np.random.binomial(1, 0.5, (1000,))\n",
    "x0_inv = [informative_feature(spurious_label(y, 0.25)) for y in y0]\n",
    "x1_inv = [informative_feature(spurious_label(y, 0.25)) for y in y1]\n",
    "x0_non_inv = [informative_feature(spurious_label(y, 0.8)) for y in y0]\n",
    "x1_non_inv = [informative_feature(spurious_label(y, 0.9)) for y in y1]\n",
    "x0 = np.concatenate((x0_inv, x0_non_inv), axis = 1)\n",
    "x1 = np.concatenate((x1_inv, x1_non_inv), axis = 1)\n",
    "#x0 = np.concatenate((x0, np.random.normal(9, 1, (1000, 60))), axis = 1)\n",
    "#x1 = np.concatenate((x1, np.random.normal(7, 1, (1000, 60))), axis = 1)\n",
    "y0 = tf.one_hot(y0, 2)\n",
    "y1 = tf.one_hot(y1, 2)\n",
    "x0 = tf.cast(x0, dtype=tf.float32)\n",
    "x1 = tf.cast(x1, dtype = tf.float32)\n",
    "\n",
    "data_test = [[x0, y0], [x1, y1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InvarLabelShift(data_train, data_test, batch_size = 250, num_steps = 2500, \n",
    "                    learning_rate = 1e-5, \n",
    "                    reg_wasserstein = 0, wasserstein_epoch = 1, \n",
    "                    gamma_wasserstein = 1e-2, \n",
    "                    reg_var = 10, sinkhorn_iter = 5, clip_grad = 10):\n",
    "    graph = utils.InvarianceNNGraph()\n",
    "    batch_data = []\n",
    "    for env in data_train:\n",
    "        batch = tf.data.Dataset.from_tensor_slices((env[0], env[1]))\n",
    "        batch = batch.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n",
    "        batch_data.append(batch.take(num_steps))\n",
    "            \n",
    "    for env in data_test:\n",
    "        batch = tf.data.Dataset.from_tensor_slices((env[0], env[1]))\n",
    "        batch = batch.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n",
    "        batch_data.append(batch.take(num_steps))\n",
    "\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    train_accuracy = tf.keras.metrics.Mean('train_accuracy', dtype=tf.float32)\n",
    "    test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "    test_accuracy = tf.keras.metrics.Mean('test_accuracy', dtype=tf.float32)\n",
    "    test_accuracy_env0 = tf.keras.metrics.Mean('train_accuracy_env:'+str(env), dtype=tf.float32)\n",
    "    test_accuracy_env1 = tf.keras.metrics.Mean('train_accuracy_env:'+str(env), dtype=tf.float32)\n",
    "    var_norm = tf.keras.metrics.Mean('total_norm', dtype=tf.float32)\n",
    "    grad_norm = tf.keras.metrics.Mean('Gradient_norm', dtype=tf.float32)\n",
    "    train_accuracy_env = dict()\n",
    "    test_accuracy_env = dict()\n",
    "    for env in [0,1]:\n",
    "        train_accuracy_env[env] = tf.keras.metrics.Mean('train_accuracy_env:'+str(env), dtype=tf.float32)\n",
    "        test_accuracy_env[env] = tf.keras.metrics.Mean('test_accuracy_env:'+str(env), dtype=tf.float32)\n",
    "        \n",
    "    train_wasserstein_y = dict()\n",
    "    test_wasserstein_y = dict()\n",
    "    for y in [0,1]:\n",
    "        train_wasserstein_y[y] = tf.keras.metrics.Mean('train_wasserstein_y:'+str(y), dtype=tf.float32)\n",
    "        test_wasserstein_y[y] = tf.keras.metrics.Mean('test_wasserstein_y:'+str(y), dtype=tf.float32)\n",
    "        \n",
    "    \n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    parameter = f'_num_steps_{num_steps}_lr_{learning_rate}_reg_wasserstein_{reg_wasserstein}_wasserstein_epoch_{wasserstein_epoch}_reg_var_{reg_var}_gamma_wasserstein_{gamma_wasserstein}_sinkhorn_iter_{sinkhorn_iter}'\n",
    "    train_log_dir = 'logs/' + current_time + parameter + '/train'\n",
    "    test_log_dir = 'logs/' + current_time + parameter + '/test'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "    \n",
    "    \n",
    "    def _accuracy(y, ypred):\n",
    "        acc = tf.cast(tf.equal(y, ypred), dtype = tf.float32)\n",
    "        return tf.reduce_mean(acc)\n",
    "\n",
    "    \n",
    "    def plot_to_image(figure):\n",
    "        \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "          returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "        # Save the plot to a PNG in memory.\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        # Closing the figure prevents it from being displayed directly inside\n",
    "        # the notebook.\n",
    "        plt.close(figure)\n",
    "        buf.seek(0)\n",
    "        # Convert PNG buffer to TF image\n",
    "        image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "        # Add the batch dimension\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        return image\n",
    "\n",
    "\n",
    "    \n",
    "    def train_step(data_train_epoch, full_data, step):\n",
    "        with tf.GradientTape() as g:\n",
    "            loss = tf.cast(0, dtype = tf.float32)\n",
    "            for index, (x, y) in enumerate(data_train_epoch):\n",
    "                probs = graph(x, env = index)\n",
    "                loss = loss + utils.EntropyLoss(y, probs)\n",
    "            loss_train = loss\n",
    "            WD = [0,0]\n",
    "            if step % wasserstein_epoch == 0:\n",
    "                for label in [0,1]:\n",
    "                    conditional_data = [env[0][env[1][:, 1] == label] for env in full_data]\n",
    "                    wasserstein_dist = sh.sinkhorn_dist(graph.invariant_map(conditional_data[0]), \n",
    "                                                                   graph.invariant_map(conditional_data[1]), \n",
    "                                                                   gamma_wasserstein, sinkhorn_iter)\n",
    "                    train_wasserstein_y[label](wasserstein_dist)\n",
    "                    #print(f'WD train y:{label} at step {step} is {wasserstein_dist}\\n')\n",
    "                    WD[label] = wasserstein_dist\n",
    "                    \n",
    "                    loss = loss + reg_wasserstein*wasserstein_dist\n",
    "                    \n",
    "                \n",
    "                    \n",
    "            #else:\n",
    "            #    for label in [0,1]:\n",
    "            #        train_wasserstein_y[y](tf.Variable(0, dtype = tf.float32))\n",
    "                    \n",
    "            variables = graph.trainable_variables\n",
    "            norm = tf.cast(0, dtype = tf.float32)\n",
    "            for v in variables:\n",
    "                norm = norm + tf.norm(v)\n",
    "            \n",
    "                \n",
    "            loss = loss + reg_var*(norm)\n",
    "            \n",
    "        trainable_variables = graph.trainable_variables\n",
    "        gradients = g.gradient(loss, trainable_variables)\n",
    "        clipped_grad = [tf.clip_by_norm(g, clip_grad) for g in gradients]\n",
    "        optimizer.apply_gradients(zip(clipped_grad, trainable_variables))\n",
    "        norm_grad = tf.cast(0, dtype = tf.float32)    \n",
    "        for v in gradients:\n",
    "            norm_grad = norm_grad + tf.norm(v, ord = 2)\n",
    "            \n",
    "        train_loss(loss_train)\n",
    "        var_norm(norm**2)\n",
    "        grad_norm((learning_rate*norm_grad))\n",
    "            \n",
    "        accuracy_train = tf.cast(0, dtype = tf.float32)\n",
    "        for index, (x, y) in enumerate(data_train_epoch):\n",
    "            predict = graph(x, env = index, predict = True)\n",
    "            #predict = tf.cast(tf.argmax(prob, axis = 1), dtype = tf.float32)\n",
    "            #with train_summary_writer.as_default():\n",
    "            #    figure = plt.hist(prob[:,1])\n",
    "            #    cm_image = plot_to_image(figure)\n",
    "            #    tf.summary.image(\"probability_hist_env:\"+str(index), cm_image, step=epoch)\n",
    "                \n",
    "            accuracy_train_env = _accuracy(y[:,1], predict)\n",
    "            accuracy_train = accuracy_train + accuracy_train_env\n",
    "            train_accuracy_env[index](accuracy_train_env)\n",
    "        accuracy_train = accuracy_train/2\n",
    "        train_accuracy(accuracy_train)\n",
    "        return WD[0], WD[1]\n",
    "            \n",
    "    def test_step(data_test_epoch, full_data, step):\n",
    "        loss = tf.cast(0, dtype = tf.float32)\n",
    "        for index, (x, y) in enumerate(data_test_epoch):\n",
    "            probs = graph(x, env = index)\n",
    "            loss = loss + utils.EntropyLoss(y, probs)\n",
    "        test_loss(loss)\n",
    "            \n",
    "        accuracy_test = tf.cast(0, dtype = tf.float32)\n",
    "        for index, (x, y) in enumerate(data_test_epoch):\n",
    "            predict = graph(x, env = index, predict = True)\n",
    "            accuracy_test_env = _accuracy(y[:,1], predict)\n",
    "            accuracy_test = accuracy_test + accuracy_test_env\n",
    "            test_accuracy_env[index](accuracy_test_env)\n",
    "        accuracy_test = accuracy_test/2\n",
    "        test_accuracy(accuracy_test)\n",
    "        WD = [0,0]\n",
    "        if step % wasserstein_epoch == 0:\n",
    "                for label in [0,1]:\n",
    "                    conditional_data = [env[0][env[1][:, 1] == label] for env in full_data]\n",
    "                    wasserstein_dist = sh.sinkhorn_dist(graph.invariant_map(conditional_data[0]), \n",
    "                                                                   graph.invariant_map(conditional_data[1]), \n",
    "                                                                   gamma_wasserstein, sinkhorn_iter)\n",
    "                    test_wasserstein_y[label](wasserstein_dist)\n",
    "                    WD[label] = wasserstein_dist\n",
    "                    \n",
    "                    #print(f'WD test y:{label} at step {step} is {wasserstein_dist}\\n')\n",
    "        #else:\n",
    "            #for label in [0,1]:\n",
    "                #test_wasserstein_y[y](tf.Variable(0, dtype = tf.float32))\n",
    "        return WD[0], WD[1]\n",
    "\n",
    "    for step, data in enumerate(zip(*batch_data), 1):\n",
    "        batch_data_train = data[:2]\n",
    "        batch_data_test = data[2:]\n",
    "        wd0_tr, wd1_tr = train_step(batch_data_train, data_train, step)\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=step)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=step)\n",
    "            for env in [0,1]:\n",
    "                tf.summary.scalar('accuracy_env:'+str(env), train_accuracy_env[env].result(), step = step)\n",
    "                \n",
    "            for y in [0,1]:\n",
    "                tf.summary.scalar('wasserstein_y:'+str(y), train_wasserstein_y[y].result(), step = step)   \n",
    "        \n",
    "        w0, w1 = test_step(batch_data_test, data_test, step)\n",
    "      \n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=step)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=step)\n",
    "            tf.summary.scalar('norm', var_norm.result(), step = step)\n",
    "            tf.summary.scalar('gradient_norm', grad_norm.result(), step = step)\n",
    "            \n",
    "            for env in [0,1]:\n",
    "                tf.summary.scalar('accuracy_env:'+str(env), test_accuracy_env[env].result(), step = step)   \n",
    "                \n",
    "            for y in [0,1]:\n",
    "                tf.summary.scalar('wasserstein_y:'+str(y), test_wasserstein_y[y].result(), step = step)   \n",
    "                \n",
    "        train_loss.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "        var_norm.reset_states()\n",
    "        grad_norm.reset_states()\n",
    "        \n",
    "        for env in [0,1]:\n",
    "            train_accuracy_env[env].reset_states()\n",
    "            test_accuracy_env[env].reset_states()\n",
    "        if step % wasserstein_epoch == 0:    \n",
    "            for y in [0,1]:\n",
    "                train_wasserstein_y[y].reset_states()\n",
    "                test_wasserstein_y[y].reset_states()\n",
    "            #print(f'Training WD for step {step} are {wd0_tr} {wd1_tr}\\n')\n",
    "            #print(f'Test WD for step {step} are {wd0_test} {wd1_test}\\n')\n",
    "        if step % 50 == 0:\n",
    "            print(f'Done step {step}\\n')\n",
    "            \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done step 50\n",
      "\n",
      "Done step 100\n",
      "\n",
      "Done step 150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = InvarLabelShift(data_train, data_test, num_steps=10000, \n",
    "                        reg_wasserstein=1, reg_var = 1e-2, learning_rate = 2e-3, \n",
    "                        wasserstein_epoch = 50, gamma_wasserstein = 1, sinkhorn_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = data_train\n",
    "for label in [0,1]:\n",
    "    conditional_data = [env[0][env[1][:, 1] == label] for env in full_data]\n",
    "    wasserstein_dist = sh.sinkhorn_dist(graph.invariant_map(conditional_data[0]), \n",
    "                                                                   graph.invariant_map(conditional_data[1]), \n",
    "                                                                   2, 10)\n",
    "    print(f'WD y:{label} is {wasserstein_dist}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(x0, env = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(x0, env = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _accuracy(y, ypred):\n",
    "        acc = tf.cast(tf.equal(y, ypred), dtype = tf.float32)\n",
    "        return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train_env = [0, 0]\n",
    "for env in [0,1]:\n",
    "    data_env = data_train[env]\n",
    "    predict = graph(data_env[0], env = env, predict = True)\n",
    "    accuracy_train_env[env] = _accuracy(data_env[1][:,1], predict)\n",
    "accuracy_train_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test_env = [0, 0]\n",
    "for env in [0,1]:\n",
    "    data_env = data_test[env]\n",
    "    predict = graph(data_env[0], env = env, predict = True)\n",
    "    accuracy_test_env[env] = _accuracy(data_env[1][:,1], predict)\n",
    "accuracy_test_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(graph.invariant_map(x0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(graph(x0, env = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = graph(x0, env = 0)\n",
    "y = graph(x0, env = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
